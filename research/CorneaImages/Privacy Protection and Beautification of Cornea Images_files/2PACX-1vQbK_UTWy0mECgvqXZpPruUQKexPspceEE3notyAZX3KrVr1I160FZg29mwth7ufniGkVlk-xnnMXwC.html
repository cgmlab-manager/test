<!DOCTYPE html>
<!-- saved from url=(0113)https://gdoc.pub/raw/doc/e/2PACX-1vQbK_UTWy0mECgvqXZpPruUQKexPspceEE3notyAZX3KrVr1I160FZg29mwth7ufniGkVlk-xnnMXwC -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Privacy Protection and Beautification of Cornea Images</title><link rel="shortcut icon" href="https://ssl.gstatic.com/docs/documents/images/kix-favicon7.ico"><meta name="referrer" content="origin"><style type="text/css" nonce="01Dxq3DQqAQXnUoaGWqigw">
      @import url("https://fonts.googleapis.com/css?family=Google+Sans");
      @import url("https://fonts.googleapis.com/css?family=Roboto");

      body {
        font-family: Roboto, arial, sans, sans-serif;
        margin: 0;
      overflow: hidden; line-height: 1.3;}

      iframe {
        border: 0;
        frameborder: 0;
        height: 100%;
        width: 100%;
      }

      #banners {
        align-items: center;
        background: white;
        display: block;
        justify-content: space-between;
        position: fixed;
        top: 0;
        width: 100%;
        z-index: 100;
      }

      #banners #publish-banner {
        background-color: #e8f0fe;
        border-bottom: 1px #ccc solid;
        color: #202124;
        display: flex;
        flex: 1 1 0%;
        height: 60px;
        width: 100%;
      }

      #publish-banner-icon {
        display: flex;
        fill: #1967d2;
        margin: auto 20px;
      }

      #publish-banner-text {
        flex-grow: 1;
        margin: auto 0;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
      }

      #publish-banner-buttons {
        margin: auto 25px auto 0;
      }

      #publish-banner-buttons span {
        align-self: center;
        background-color: inherit;
        border: none;
        font-family: "Google Sans", Roboto, RobotoDraft, Helvetica, Arial, sans-serif;
        margin: 0 16px 0 32px;
      }

      #publish-banner-buttons a {
        color: #1967d2;
        cursor: pointer;
        font-family: "Google Sans", Roboto, RobotoDraft, Helvetica, Arial, sans-serif;
        font-size: 14px;
        font-weight: 500;
        line-height: 24px;
        text-decoration: none;
      }

      #banners #title-banner {
        background: white;
        border-bottom: 1px #ccc solid;
        display: flex;
        flex: 1 1 0%;
        height: 60px;
        width: 100%;
      }

      #banners #title-banner #title {
        flex-grow: 1;
        font-family: 'Google Sans';
        font-size: large;
        margin: auto 0 auto 20px;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
        width: 70%;
      }

      #banners #title-banner #interval {
        margin: auto 25px auto 0;
        font-family: Roboto;
        font-size: small;;
      }

      #footer {
        background: #f0f0f0;
        border-bottom: 1px #ccc solid;
        bottom: 0;
        font-family: Roboto;
        font-size: small;
        padding: 10px 10px;
        position: fixed;
        text-align: center;
        width: 100%;
      }

      #contents {
        padding: 100px 20% 50px 20%;
      }

      @media only screen and (max-device-width: 800px) {
        #banners #title-banner {
          border-bottom-width: 5px;
          height: auto;
          display: block;
        }

        #banners #title-banner #title {
          font-size: 3em;
          margin: auto 0 auto 20px;
          width: 90%;
        }

        #banners #title-banner #interval {
          font-size: 1.5em;
          margin: 10px 0 auto 25px;
        }

        #contents {
          padding: 150px 5% 80px;
        }

        #footer {
          font-size: 2em;
        }
      }

      .dash {
        padding: 0 6px;
      }
    @media print { table td, table th { page-break-inside: avoid; } }</style></head><body><div><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c4{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c7{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c11{padding-top:1pt;padding-bottom:6pt;line-height:1.0999999999999999;orphans:2;widows:2;text-align:center}.c9{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c8{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style><div class="c9"><p class="c7 title" id="h.bdrrelkch8ns"><span class="c6">Privacy Protection and Beautification of Cornea Images</span></p><p class="c5"><span class="c3">Chia-Lin Wu and Chuan-Kai Yang and Yi-Ling Lin</span></p><p class="c5"><span class="c3">Department of Information Management</span></p><p class="c5"><span class="c3">National Taiwan University of Science and Technology</span></p><p class="c5 c8"><span class="c3"></span></p><p class="c5"><span class="c3">No. 43, Sec. 4, Keelung Road</span></p><p class="c5"><span class="c3">Taipei, 106, Taiwan, ROC</span></p><p class="c5 c8"><span class="c3"></span></p><p class="c5"><span class="c3">cruisewu98@gmail.com,ckyang@cs.ntust.edu.tw,elina10920@gmail.com</span></p><h2 class="c4" id="h.4yt01ute9jut"><span class="c1">ABSTRACT</span></h2><p class="c10"><span class="c3">Thanks to technological advances, social media has become more popular year by year, especially when it is common to upload selfies to the Internet where anyone from anywhere can have access, thus leading to some privacy issues. More specifically, when a selfie photo is relatively clear and bright, there could be a high probability of revealing a personâ€™s locations and/or some associated information. In this paper, we design a framework to automatically obtain cornea information. First, we apply the Haar Cascade algorithm on the captured eye area and YOLO object detector for cornea location. Next, we calibrate the image to get a more accurate identification, where Google Vision API is used for recognition. Experimental results indicate that we may get some privacy information from a photo. Although lowering image quality could help reduce the risk of privacy exposure, it could make the photo undesirably blur. We propose a novel method to remove sensitive privacy information while at the same time being able to produce eye-stunning images.</span></p><h2 class="c4" id="h.n8kp3ry6c1bd"><span class="c1">System Flow</span></h2><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 260.00px;"><img alt="" src="./pasted image 0.png" style="width: 601.70px; height: 260.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c3">As shown in Figure 3, our proposed system consists of four portions: cornea extraction, image correction and restoration, risk analysis and cornea de-identification. We use selfie software to take pictures with a cellphone as inputs. Our system first detects the eye area from an input image using the Haar Cascade algorithm. Then we fine-tune the YOLO model to achieve cornea extraction. Once the cornea location is obtained, a fisheye correction is used to transform the circular image into a rectangular image. By using EDSR to enlarge and FFDNet to remove the noise from the small-sized image, we then analyze risk factors with Google Vision API as the result. Finally we relocate the light of the image with some patterns that we design to remove the potential sensitive information.</span></p><h2 class="c4" id="h.6aphcdrk3opi"><span class="c1">User Case</span></h2><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 444.00px; height: 263.00px;"><img alt="" src="./pasted image 0(1).png" style="width: 444.00px; height: 263.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 149.73px; height: 166.67px;"><img alt="" src="./Untitled.jpg" style="width: 149.73px; height: 166.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 127.65px; height: 170.89px;"><img alt="" src="./pasted image 0(2).png" style="width: 127.65px; height: 170.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 574.00px; height: 177.45px;"><img alt="" src="./pasted image 0(3).png" style="width: 574.00px; height: 198.00px; margin-left: 0.00px; margin-top: -20.55px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 238.00px; height: 184.00px;"><img alt="" src="./pasted image 0(4).png" style="width: 238.00px; height: 184.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4" id="h.1e8vta7772o1"><span class="c1">Cornea Extraction</span></h2><p class="c2"><span class="c3">The cornea is the transparent part of the surface of an eye, which can reflect objects like a convex mirror [12]. As shown in Figure 4, we need to get the position of the cornea for subsequent processing. The process in this section is to detect the human eye area in the</span></p><p class="c2"><span class="c3">photo from a selfie or a photo taken by others, and obtain the cornea position from it for the use in next subsections. The process is divided into the following two parts, (1) human eye area, and (2) cornea area.</span></p><p class="c5 c8"><span class="c3"></span></p><h2 class="c4" id="h.6msdxltb6292"><span class="c1">Eye Area Capture</span></h2><p class="c2"><span class="c3">For obtaining the cornea position in a photograph, we use an effective object detection method proposed by Paul &amp; Michael [30] in 2011, based on the Haar Cascade classifier to detect the face and eyes of a person and mark the position. We use OpenCV library,</span></p><p class="c2"><span class="c3">which provides a pre-trained Haar Cascade model to detect the face in an image, and also the pre-trained Haar Cascade model to detect the position of the eyes in the image. The more specific flow is as follows. First, a large number of positive images (face images)</span></p><p class="c2"><span class="c3">and negative images (non-face images) are required to extract the Haar-like features of a face. In addition to the features proposed by [30], Lienhart &amp; Maydt [20] subsequently extended the 45â—¦ rotated feature, resulting totally 14 Haar features to be used. Integral</span></p><p class="c0"><span class="c3"></span></p><p class="c2"><span class="c3">Image is used to accelerate the computation of the feature and sent to the AdaBoost algorithm [6] to train a strong classifier that distinguishes between human faces and non-faces, and concatenate several strong classifiers in series to detect images more accurately.</span></p><p class="c2"><span class="c3">The result is shown in Figure 5. We could discover that although the faces are with slightly different angles, the eye areas can still be captured correctly.</span></p><p class="c0"><span class="c3"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 544.00px;"><img alt="" src="./pasted image 0(5).png" style="width: 601.70px; height: 544.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 570.67px;"><img alt="" src="./pasted image 0(6).png" style="width: 601.70px; height: 570.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c2"><span class="c3">Currently OpenCV does not provide a pre-trained model for cornea detection. If we want to use Haar Cascade Classifier to train the cornea position, a large number of labeled data sets are required.</span></p><p class="c0"><span class="c3"></span></p><p class="c2"><span class="c3">For this, we fine-tune the pre-trained model of YOLOv4 to detect cornea, as it can reduce the number of training data to achieve good results. When taking pictures outdoors, we will face a variety of different illuminations. Therefore, data augmentation by color transformation of the data during training is necessary. For this purpose, we randomly adjust the brightness of an image by increasing or decreasing the brightness of 25% to allow the model to adapt to various lighting environments. The result is shown in Figure 6. All corneas in eye images are captured accurately.</span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 433.00px; height: 379.00px;"><img alt="" src="./pasted image 0(7).png" style="width: 433.00px; height: 379.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h2 class="c4" id="h.etxn19za5uwp"><span class="c1">Fisheye Corection</span></h2><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.15px; height: 425.26px;"><img alt="" src="./pasted image 0(8).png" style="width: 312.15px; height: 425.26px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4" id="h.s5d8jma2rb07"><span class="c1">Super Resolution</span></h2><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 426.00px; height: 514.00px;"><img alt="" src="./pasted image 0(9).png" style="width: 426.00px; height: 514.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4" id="h.8wkxcbyxrfzs"><span class="c1">Object Detection From Corea Images</span></h2><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 434.00px; height: 501.00px;"><img alt="" src="./pasted image 0(10).png" style="width: 434.00px; height: 501.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4" id="h.qtv1sx3t7t5f"><span class="c1">Risk Analysis</span></h2><p class="c2"><span class="c3">In this section, we show some experiments we performed for risk analysis regarding the factors we considered. We analyzed all the data that we collected and demonstrated the scatter plot in 3D with distance, light and blurriness values. All the 3D scatters follow the coordinate system shown in Figure 17.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 424.00px;"><img alt="" src="./pasted image 0(11).png" style="width: 601.70px; height: 424.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 438.67px;"><img alt="" src="./pasted image 0(12).png" style="width: 601.70px; height: 438.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 445.33px;"><img alt="" src="./pasted image 0(13).png" style="width: 601.70px; height: 445.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4" id="h.u4zjwuxpjry3"><span class="c1">Cornea De-identification</span></h2><p class="c2"><span class="c3">First, we find the brightest area of the face by using image binarization on the grayscale im-</span></p><p class="c2"><span class="c3">age and find the center point from it. We then use the center point of the eye and the center point of the brightest area to calculate a straight line, and place the bright points on that straight line with the distance of half the cornea radius from the center points of the eye (As shown in Equation 4).</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 476.00px; height: 109.00px;"><img alt="" src="./pasted image 0(14).png" style="width: 476.00px; height: 109.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4" id="h.b4amvg87cq93"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 342.67px;"><img alt="" src="./pasted image 0(15).png" style="width: 601.70px; height: 342.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 318.67px;"><img alt="" src="./pasted image 0(16).png" style="width: 601.70px; height: 318.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 298.67px;"><img alt="" src="./pasted image 0(17).png" style="width: 601.70px; height: 298.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h2 class="c4" id="h.wfdmhic0t3h1"><span class="c1">CONCLUSION AND FUTURE WORK</span></h2><p class="c0"><span class="c3"></span></p><p class="c2"><span class="c3">In this paper, we analyze the risks and the potential impacts of portrait photos, where the image reflected on cornea may disclose private information. We use two different mobile phones to capture data taken outdoors, and applied the image processing procedures</span></p><p class="c2"><span class="c3">we proposed. Our suggested iris localization method has achieved high accuracy within the UBIRIS database which means it is with high probability of being recognized. And we also propose an eye beautification method for de-identification as a defense against future incidents. We have found that distance is the most influential factor in taking photos. The experimental results also show that by increasing the exposure value of a mobile phone, we can identify people from an original distance of 50(cm) to the distance of 75(cm), but the recognition probability is still not high. And it is rarely found that a distance of more than 70(cm) can be used to identify people. For now, the probability of being identified is still not very high. However, considering the improvement of the mobile phone camera hardware and software, it is necessary to regard the issue of exposing privacy when shooting and uploading photos.</span></p><p class="c2"><span class="c3">In the future, our analysis method could be applied to a variety of latest mobile phones and the distance of risk can be set for each mobile phone according to its specification as a reminder for a user before updating photos on to the Internet.</span></p><p class="c0"><span class="c3"></span></p></div></div></body></html>