<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Realistic Video Generation for American Sign Language</title>
    <link rel="stylesheet" href="https://latex.now.sh/style.css" />
    <style>
      figure {
        padding: 16px;
      }
      figcaption {
        font-weight: bold;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>Realistic Video Generation for American Sign Language</h1>
      <p class="author">
        Meng-Chen Xu and Chuan-Kai Yang
        <br />
        Department of Information Management
        <br />
        National Taiwan University of Science and Technology No. 43, Sec. 4,
        Keelung Road Taipei, 106, Taiwan, ROC
        <br />
        <a href="mailto:B10509017@gapps.ntust.edu.tw">
          B10509017@gapps.ntust.edu.tw
        </a>
        ,
        <a href="mailto:ckyang@cs.ntust.edu.tw">ckyang@cs.ntust.edu.tw</a>
      </p>
    </header>
    <div class="abstract">
      <h2>Abstract</h2>
      <p>
        <small>
          There are many ways to generate sign language videos, but most of them
          are based on 3D character modeling. These methods are time-consuming
          and labor-intensive, and are hard to compare with the videos of real
          person sign language in terms of realness and naturalness. To address
          this, we propose a novel approach using the recently popular
          generative adversarial network to synthesize sentence-level videos
          from word-level videos. A pose transition estimation is used to
          measure the distance between sign language clips and synthesize the
          corresponding transition skeletons. In particular, we propose an
          interpolation approach, as it is faster than a graphics approach and
          does not require additional datasets. In addition, we also propose an
          stacked based approach for the Vid2Vid model. Two Vid2Vid models are
          stacked together to generate videos via two stages. The first stage is
          to generate IUV images (3 channels images composed by the index I and
          the UV texture coordinates) from the skeleton images, and the second
          stage is to generate realistic video from the skeleton images and the
          IUV images. We use American Sign Language Lexicon Video Dataset
          (ASLLVD) in our experiment, and we found that when the skeletons are
          generated by our proposed pose transition estimation method, the
          quality of the video is better than that of the direct generation
          using only the skeletons. Finally, we also develop a graphical user
          interface that allows users to drag and drop the clips to the video
          track and generate a realistic sign language video.
        </small>
      </p>
    </div>
    <div class="proposed-system">
      <h2>Proposed System</h2>
      <div class="system-overview">
        <h3>System Overview</h3>
        <figure>
          <img src="system.png" alt="" />
        </figure>
        <figcaption>
          Figure: System Flow Chart. Input of this system are gloss words,
          followed by querying a database to retrieve corresponding clips, and
          then to extract skeleton keypoints and IUV images via OpenPose and
          DensePose models and the output is a corresponding realistic video.
        </figcaption>
      </div>
      <div class="web-graphical-user-interface">
        <h3>Web Graphical User Interface</h3>
        <figure>
          <img src="web.png" alt="" />
        </figure>
        <figcaption>
          Figure: A graphical web user interface of word-level to sentence-level
          realistic sign language video generation system.
        </figcaption>
      </div>
    </div>
    <div class="results">
      <h2>Results</h2>
      <div class="compare-with-others">
        <h4>Comparison with other methods</h4>
        <figure>
          <img src="https://sheiun.me/w2s-slg/assets/compare.gif" alt="" />
        </figure>
        <figcaption>
          Figure: We compare our approach with others’ method on synthesizing
          the “awkward baby” sign language video. Currently, we only compare to
          the method proposed by Yulia et al. since we use the same dataset and
          we could obtain their video results.
        </figcaption>
      </div>
    </div>
    <div class="conclusion">
      <h2>Conclusion</h2>
      <p>
        <small>
          We presented a text-to-sign language realistic video generation system
          that can synthesize sentence-level videos from word-level clips. This
          is achieved by our stacked models scheme which consists of skeleton
          images to IUV image synthesis and skeleton image & IUV image to
          realistic image synthesis. The proposed system has addressed the
          problem of single-hand and two-hands motion transition as faced in
          Yulia et al.’s work and addressed the consistency of styles like dress
          color, hair style or even background color. And we proposed an
          algorithm to prevent different style images being generated through
          models by constraining the input data (i.e. skeleton correction). We
          found that generating a full video makes better style consistency than
          the concatenation of real clips and the generation of transition
          clips, because part associations estimated by OpenPose sometimes may
          fail when two hands are crossed or when one of the hands is blurry.
          The pattern is also learned by the models, and therefore, we apply
          skeleton correction before synthesizing a video. The skeleton images
          to IUV images synthesis could be considered successful if we use human
          eyes to assess the quality, but evaluation metrics show that the
          quality is not very high, compared with other realistic synthesis
          approaches. We found that the length of the knuckles can be used to
          determine whether the pose of a hand is irregular or abnormal. And we
          proposed a method for correcting irregular hand poses.
        </small>
      </p>
    </div>
  </body>
</html>
